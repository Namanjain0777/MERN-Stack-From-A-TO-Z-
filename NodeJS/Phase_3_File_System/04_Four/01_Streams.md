# ğŸš€ What Are Streams?

A **stream** is a way to handle large data **chunk by chunk** instead of loading everything into memory.

Instead of:

âŒ Load entire file (1GB) into RAM
We do:
âœ… Read small chunks and process gradually

---

# ğŸ§  Why Streams Matter

Imagine:

* 2GB video file
* 500MB CSV
* Live video streaming
* Real-time logs

If you use `readFile()`:

ğŸš¨ Entire file loads into memory â†’ crash risk

If you use Streams:

âš¡ Process small pieces â†’ memory efficient
âš¡ Faster
âš¡ Scalable

Thatâ€™s why:

> Netflix, YouTube, large APIs use streams.

---

# ğŸ“¦ Types of Streams in Node

There are 4 types:

1. Readable
2. Writable
3. Duplex
4. Transform

Today we focus on:

âœ” Readable
âœ” Writable
âœ” Piping

---

# 1ï¸âƒ£ Readable Streams

ğŸ‘‰ Used to read data in chunks.

Import:

```js
import fs from "fs";
```

---

## ğŸ”¹ Example: Read File Using Stream

```js
const readStream = fs.createReadStream("bigfile.txt", "utf8");

readStream.on("data", (chunk) => {
  console.log("New chunk received:");
  console.log(chunk);
});

readStream.on("end", () => {
  console.log("Finished reading file ğŸš€");
});
```

---

### ğŸ”¥ What Happens?

Instead of loading full file:

Node reads:

```
Chunk 1
Chunk 2
Chunk 3
...
```

---

# 2ï¸âƒ£ Writable Streams

ğŸ‘‰ Used to write data in chunks.

---

## ğŸ”¹ Example

```js
const writeStream = fs.createWriteStream("output.txt");

writeStream.write("Hello ");
writeStream.write("Naman ğŸš€");

writeStream.end();
```

This writes data gradually.

---

# 3ï¸âƒ£ Piping (MOST IMPORTANT)

ğŸ”¥ This is where streams become powerful.

Piping connects:

Readable â†’ Writable

Without storing full data in memory.

---

## ğŸ”¹ Example: Copy Large File

```js
const readStream = fs.createReadStream("bigfile.txt");
const writeStream = fs.createWriteStream("copy.txt");

readStream.pipe(writeStream);
```

ğŸ”¥ Thatâ€™s it.

No manual chunk handling needed.

---

# ğŸ§  Why pipe() is Powerful?

Instead of:

```js
fs.readFile()
fs.writeFile()
```

Which loads everything into memory,

pipe():

* Reads chunk
* Sends to destination
* Reads next chunk
* Memory efficient

---

# ğŸ“Š readFile vs Stream Comparison

| Feature             | readFile | Streams |
| ------------------- | -------- | ------- |
| Memory usage        | High     | Low     |
| Large files         | Risky    | Safe    |
| Performance         | Slower   | Faster  |
| Real-time streaming | No       | Yes     |

---

# ğŸš€ Real Backend Use Cases

Streams are used in:

* File uploads
* Video streaming
* Sending large CSV exports
* Logging systems
* HTTP responses

---

# ğŸŒ HTTP + Streams Example

```js
import http from "http";
import fs from "fs";

http.createServer((req, res) => {
  const readStream = fs.createReadStream("video.mp4");
  readStream.pipe(res);
}).listen(3000);
```

ğŸ”¥ This streams video directly to browser.

No memory crash.

---

# ğŸ§  Interview Questions

### â“ What is a stream?

A method of processing data in chunks instead of loading it fully into memory.

### â“ Why are streams important?

They improve memory efficiency and performance for large data.

### â“ What is pipe()?

Method to connect readable stream to writable stream.

### â“ Difference between readFile & createReadStream?

readFile loads entire file; stream reads in chunks.

---

# ğŸ§  Deep Understanding (Event Loop Connection)

Streams:

* Work with event-driven model
* Emit events like:

  * data
  * end
  * error

Nodeâ€™s architecture is built around streams.

---

